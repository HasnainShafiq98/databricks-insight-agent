# ============================================================================
# Databricks Insight Agent - Environment Configuration
# ============================================================================
# Copy this file to .env and update with your actual credentials
# IMPORTANT: Never commit .env to version control!

# ============================================================================
# Databricks Configuration (REQUIRED)
# ============================================================================
# Your Databricks workspace URL (without https://)
DATABRICKS_SERVER_HOSTNAME=your-workspace.cloud.databricks.com

# SQL Warehouse HTTP path - Find in: SQL Warehouses → Your Warehouse → Connection Details
DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/your-warehouse-id

# Personal Access Token - Generate in: User Settings → Access Tokens
DATABRICKS_ACCESS_TOKEN=dapi1234567890abcdefghijklmnopqrstuvwxyz

# Catalog and Schema (Unity Catalog)
DATABRICKS_CATALOG=hive_metastore
DATABRICKS_SCHEMA=default

# ============================================================================
# OpenAI Configuration (OPTIONAL - for enhanced query understanding)
# ============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your-api-key-here

# Model selection
OPENAI_MODEL=gpt-4-turbo-preview

# ============================================================================
# FAISS Configuration (Vector Search)
# ============================================================================
# Path to store FAISS index (local or DBFS)
FAISS_INDEX_PATH=./data/faiss_index.faiss

# Directory containing knowledge base documents
DOCUMENTS_PATH=./data/documents

# Embedding model (Sentence Transformers)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# ============================================================================
# DBFS Configuration (for Databricks deployment)
# ============================================================================
# DBFS mount point for FAISS index storage
DBFS_MOUNT_POINT=/dbfs/mnt/faiss

# Local cache directory (when not in Databricks)
LOCAL_CACHE_DIR=./data/cache

# ============================================================================
# Security Configuration
# ============================================================================
# Maximum allowed query length (characters)
MAX_QUERY_LENGTH=10000

# API rate limiting (requests per minute per user)
RATE_LIMIT_PER_MINUTE=60

# Allowed database schemas (comma-separated)
ALLOWED_SCHEMAS=default,analytics,production

# Enable SQL injection protection
ENABLE_SQL_INJECTION_PROTECTION=true

# Enable rate limiting
ENABLE_RATE_LIMITING=true

# ============================================================================
# Data Pipeline Configuration
# ============================================================================
# CSV input directory
CSV_INPUT_DIR=./data/csv

# Enable automatic schema inference
AUTO_SCHEMA_INFERENCE=true

# Delta table optimization frequency (hours)
OPTIMIZE_FREQUENCY_HOURS=24

# Vacuum retention period (hours)
VACUUM_RETENTION_HOURS=168

# ============================================================================
# RAG Configuration
# ============================================================================
# Document chunk size (characters)
CHUNK_SIZE=512

# Chunk overlap (characters)
CHUNK_OVERLAP=50

# Top-K results for context retrieval
CONTEXT_TOP_K=3

# Minimum similarity score (0.0 to 1.0)
MIN_SIMILARITY_SCORE=0.5

# ============================================================================
# SQL Generation Configuration
# ============================================================================
# Maximum number of retries for failed queries
MAX_SQL_RETRIES=3

# Enable auto-correction for SQL errors
ENABLE_SQL_AUTO_CORRECTION=true

# Maximum rows to return (safety limit)
MAX_RESULT_ROWS=10000

# Query timeout (seconds)
QUERY_TIMEOUT_SECONDS=300

# ============================================================================
# Logging Configuration
# ============================================================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path
LOG_FILE_PATH=./logs/agent.log

# Enable console logging
ENABLE_CONSOLE_LOGGING=true

# Enable file logging
ENABLE_FILE_LOGGING=true

# Log rotation (max file size in MB)
LOG_MAX_SIZE_MB=10

# Number of backup log files
LOG_BACKUP_COUNT=5

# ============================================================================
# Streamlit Configuration (for web UI)
# ============================================================================
# Server address
STREAMLIT_SERVER_ADDRESS=localhost

# Server port
STREAMLIT_SERVER_PORT=8501

# Enable CORS
STREAMLIT_ENABLE_CORS=false

# Theme
STREAMLIT_THEME=light

# ============================================================================
# Development Configuration
# ============================================================================
# Enable debug mode
DEBUG_MODE=false

# Use mock data (when Databricks not available)
USE_MOCK_DATA=false

# Enable profiling
ENABLE_PROFILING=false

# ============================================================================
# Monitoring Configuration
# ============================================================================
# Enable metrics collection
ENABLE_METRICS=true

# Metrics export path
METRICS_PATH=./data/metrics

# Health check interval (seconds)
HEALTH_CHECK_INTERVAL=60
