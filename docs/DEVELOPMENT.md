# Development Guide

## ðŸ“š Table of Contents
1. [Project Structure](#project-structure)
2. [Architecture Deep Dive](#architecture-deep-dive)
3. [Component Details](#component-details)
4. [Development Workflow](#development-workflow)
5. [Testing Strategy](#testing-strategy)
6. [Deployment](#deployment)
7. [Best Practices](#best-practices)

---

## Project Structure

```
databricks-insight-agent/
â”‚
â”œâ”€â”€ Core Application Files
â”‚   â”œâ”€â”€ app.py                          # Streamlit web UI
â”‚   â”œâ”€â”€ main.py                         # CLI entry point
â”‚   â”œâ”€â”€ agent.py                        # Main orchestrator
â”‚   â”œâ”€â”€ examples.py                     # Example queries
â”‚   â””â”€â”€ example_usage.py                # Complete demo script
â”‚
â”œâ”€â”€ Data Layer
â”‚   â”œâ”€â”€ databricks_client.py            # Databricks SQL connector
â”‚   â”œâ”€â”€ data_pipeline.py                # Bronze/Silver/Gold pipeline
â”‚   â””â”€â”€ dbfs_integration.py             # DBFS storage management
â”‚
â”œâ”€â”€ Intelligence Layer
â”‚   â”œâ”€â”€ sql_generator.py                # Schema-aware SQL generation
â”‚   â”œâ”€â”€ sql_error_correction.py         # Auto-correction & retry
â”‚   â”œâ”€â”€ context_retriever.py            # FAISS-based RAG
â”‚   â””â”€â”€ document_processor.py           # Document chunking
â”‚
â”œâ”€â”€ Security Layer
â”‚   â””â”€â”€ security.py                     # Validation & rate limiting
â”‚
â”œâ”€â”€ Testing & Documentation
â”‚   â”œâ”€â”€ test_core.py                    # Unit tests
â”‚   â”œâ”€â”€ README.md                       # Project overview
â”‚   â”œâ”€â”€ ARCHITECTURE.md                 # Architecture details
â”‚   â”œâ”€â”€ SECURITY.md                     # Security documentation
â”‚   â”œâ”€â”€ SETUP_GUIDE.md                  # Complete setup guide
â”‚   â””â”€â”€ DEVELOPMENT.md                  # This file
â”‚
â”œâ”€â”€ Configuration
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â”œâ”€â”€ .env.example                    # Environment template
â”‚   â”œâ”€â”€ .gitignore                      # Git ignore rules
â”‚   â””â”€â”€ .env                            # Local config (not in git)
â”‚
â””â”€â”€ Data Directory
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ csv/                        # Input CSV files
    â”‚   â”œâ”€â”€ documents/                  # Knowledge base documents
    â”‚   â”œâ”€â”€ cache/                      # Local FAISS cache
    â”‚   â””â”€â”€ faiss_index.faiss           # Vector index
    â””â”€â”€ logs/                           # Application logs
```

---

## Architecture Deep Dive

### 1. Query Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Input Layer                          â”‚
â”‚                  (Streamlit UI / CLI / API)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Security Gateway                            â”‚
â”‚  â€¢ Input validation      â€¢ Rate limiting                        â”‚
â”‚  â€¢ SQL injection check   â€¢ Schema validation                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Query Analyzer (Agent)                        â”‚
â”‚  â€¢ Intent detection      â€¢ Filter identification                â”‚
â”‚  â€¢ Confidence scoring    â€¢ Missing info detection               â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                      â”‚                    â”‚
      â–¼                      â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SQL    â”‚          â”‚ Context  â”‚         â”‚  Hybrid  â”‚
â”‚   Only   â”‚          â”‚   Only   â”‚         â”‚   Mode   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                     â”‚                     â”‚
     â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”
â”‚ SQL Generator    â”‚  â”‚ FAISS Retriever  â”‚  â”‚Both â”‚
â”‚ â€¢ Schema check   â”‚  â”‚ â€¢ Embed query    â”‚  â””â”€â”€â”¬â”€â”€â”˜
â”‚ â€¢ Build query    â”‚  â”‚ â€¢ Vector search  â”‚     â”‚
â”‚ â€¢ Validate       â”‚  â”‚ â€¢ Rank results   â”‚     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
     â”‚                     â”‚                    â”‚
     â–¼                     â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                    â”‚
â”‚ Error Corrector  â”‚       â”‚                    â”‚
â”‚ â€¢ Detect error   â”‚       â”‚                    â”‚
â”‚ â€¢ Apply fix      â”‚       â”‚                    â”‚
â”‚ â€¢ Retry query    â”‚       â”‚                    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚                    â”‚
     â”‚                     â”‚                    â”‚
     â–¼                     â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Databricks Query Executor            â”‚
â”‚  â€¢ Execute SQL    â€¢ Fetch results               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Insight Generator                     â”‚
â”‚  â€¢ Combine SQL + Context                        â”‚
â”‚  â€¢ Generate narrative                           â”‚
â”‚  â€¢ Format response                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                User Response                     â”‚
â”‚  â€¢ Insights      â€¢ SQL query                    â”‚
â”‚  â€¢ Results       â€¢ Context                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Delta Table Pipeline

```
CSV Files (Local/DBFS)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BRONZE LAYER                     â”‚
â”‚  â€¢ Raw data ingestion                   â”‚
â”‚  â€¢ Minimal validation                   â”‚
â”‚  â€¢ Add metadata (_ingestion_timestamp)  â”‚
â”‚  â€¢ Schema: Inferred from CSV            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         Data Cleaning & Validation
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SILVER LAYER                     â”‚
â”‚  â€¢ Data quality checks                  â”‚
â”‚  â€¢ Type normalization                   â”‚
â”‚  â€¢ Duplicate removal                    â”‚
â”‚  â€¢ Standardized formats                 â”‚
â”‚  â€¢ Constraints enforcement              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         Business Logic & Aggregation
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GOLD LAYER                       â”‚
â”‚  â€¢ Business metrics                     â”‚
â”‚  â€¢ KPI calculations                     â”‚
â”‚  â€¢ Denormalized for performance         â”‚
â”‚  â€¢ Ready for analytics                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. RAG Pipeline

```
Documents (Text/MD files)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document Chunker      â”‚
â”‚  â€¢ Split by paragraphs  â”‚
â”‚  â€¢ Maintain context     â”‚
â”‚  â€¢ Add metadata         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embedding Model       â”‚
â”‚  (all-MiniLM-L6-v2)     â”‚
â”‚  â€¢ Convert to vectors   â”‚
â”‚  â€¢ 384 dimensions       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FAISS Index           â”‚
â”‚  â€¢ Vector storage       â”‚
â”‚  â€¢ L2 distance metric   â”‚
â”‚  â€¢ Stored in DBFS       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     Query Processing
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Similarity Search     â”‚
â”‚  â€¢ Embed query          â”‚
â”‚  â€¢ Find top-k nearest   â”‚
â”‚  â€¢ Return with scores   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### Agent (agent.py)

**Responsibility:** Main orchestration and decision-making

**Key Methods:**
- `process_query()`: Main entry point for query processing
- `analyze_query()`: Determines query intent and requirements
- `_generate_safe_sql()`: Coordinates SQL generation with validation
- `_generate_insights()`: Combines results and context into insights

**Decision Logic:**
```python
if query requires data:
    if query has clear metrics:
        â†’ SQL_ONLY
    elif query asks about definitions:
        â†’ CONTEXT_ONLY
    elif query needs both data and explanation:
        â†’ BOTH
else:
    if missing critical information:
        â†’ CLARIFICATION
```

### SQL Generator (sql_generator.py)

**Responsibility:** Schema-aware SQL generation

**Key Features:**
- Only uses columns that exist in schema
- Builds parameterized queries
- Supports filters, aggregations, joins
- Never hallucinates column names

**Example:**
```python
sql_generator.generate_sql(
    table_name="sales",
    columns=["region", "amount"],
    aggregations={"total": "SUM(amount)"},
    group_by=["region"],
    order_by=[("total", "DESC")],
    limit=10
)
```

### SQL Error Corrector (sql_error_correction.py)

**Responsibility:** Auto-correction of SQL errors

**Correction Strategies:**
1. **Column Name Typos**: Uses Levenshtein distance to find similar columns
2. **Table Name Errors**: Suggests closest matching table
3. **Syntax Errors**: Fixes common patterns (missing commas, quotes)
4. **Type Mismatches**: Adds explicit CAST statements
5. **GROUP BY Issues**: Automatically adds missing columns

**Example:**
```python
# Original (wrong)
SELECT transactoin_id FROM sales

# Auto-corrected
SELECT transaction_id FROM sales
```

### Context Retriever (context_retriever.py)

**Responsibility:** Semantic document retrieval using FAISS

**Features:**
- Embedding generation with SentenceTransformers
- Efficient vector search
- Persistent index storage
- Metadata filtering

**Usage:**
```python
retriever = ContextRetriever()
retriever.add_documents(documents)
results = retriever.search("What is CLV?", top_k=3)
```

### Data Pipeline (data_pipeline.py)

**Responsibility:** ETL from CSV to Delta tables

**Bronze Table Creation:**
```sql
CREATE TABLE bronze_sales
AS SELECT *, 
   current_timestamp() as _ingestion_timestamp,
   'source.csv' as _source_file
FROM csv_data
```

**Silver Table Creation:**
```sql
CREATE TABLE silver_sales
AS SELECT
   TRIM(transaction_id) as transaction_id,
   CAST(amount AS DECIMAL(10,2)) as amount,
   TO_DATE(date, 'yyyy-MM-dd') as date
FROM bronze_sales
WHERE amount > 0 
  AND transaction_id IS NOT NULL
```

**Gold Table Creation:**
```sql
CREATE TABLE gold_sales_by_region
AS SELECT
   region,
   SUM(amount) as total_sales,
   COUNT(*) as transaction_count,
   AVG(amount) as avg_transaction
FROM silver_sales
GROUP BY region
```

---

## Development Workflow

### Setting Up Development Environment

```bash
# 1. Clone and setup
git clone https://github.com/HasnainShafiq98/databricks-insight-agent.git
cd databricks-insight-agent
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Configure environment
cp .env.example .env
# Edit .env with your credentials

# 3. Create development branch
git checkout -b dev
git push -u origin dev

# 4. Create feature branch
git checkout -b feature/my-new-feature
```

### Git Workflow

#### Branch Structure
```
main            # Production releases only
  â””â”€â”€ dev       # Integration branch
       â”œâ”€â”€ feature/add-metric
       â”œâ”€â”€ feature/new-datasource
       â”œâ”€â”€ bugfix/sql-error
       â””â”€â”€ hotfix/security-patch
```

#### Making Changes

```bash
# 1. Update from dev
git checkout dev
git pull origin dev

# 2. Create feature branch
git checkout -b feature/add-customer-churn

# 3. Make changes and commit frequently
git add .
git commit -m "feat: Add customer churn prediction

- Implement churn calculation logic
- Add unit tests
- Update documentation"

# 4. Push to remote
git push origin feature/add-customer-churn

# 5. Create Pull Request
# Go to GitHub and create PR: feature/add-customer-churn â†’ dev

# 6. After PR approval, merge to dev
git checkout dev
git pull origin dev

# 7. Periodically merge dev to main for releases
git checkout main
git merge dev
git tag -a v1.2.0 -m "Release v1.2.0: Add customer churn analysis"
git push origin main --tags
```

#### Commit Message Convention

```
<type>(<scope>): <subject>

<body>

<footer>
```

**Types:**
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting)
- `refactor`: Code refactoring
- `test`: Adding tests
- `chore`: Build/tooling changes

**Examples:**
```bash
feat(sql): Add support for window functions

Implement ROW_NUMBER, RANK, and DENSE_RANK functions
in SQL generator with proper partitioning support.

Closes #123
```

```bash
fix(security): Prevent SQL injection in ORDER BY clause

Added validation for ORDER BY column names to prevent
injection through sorting parameters.

Security issue reported by @username
```

---

## Testing Strategy

### Unit Tests

```python
# test_core.py
import pytest
from sql_generator import SQLGenerator, SchemaManager, TableSchema

def test_sql_generation():
    schema_manager = SchemaManager()
    schema_manager.add_table(TableSchema(
        name="sales",
        columns=["id", "amount", "region"],
        column_types={"id": "STRING", "amount": "DECIMAL", "region": "STRING"}
    ))
    
    generator = SQLGenerator(schema_manager)
    sql = generator.generate_sql(
        table_name="sales",
        columns=["region"],
        aggregations={"total": "SUM(amount)"},
        group_by=["region"]
    )
    
    assert "SELECT region, SUM(amount) as total" in sql
    assert "GROUP BY region" in sql
```

### Integration Tests

```python
# tests/test_integration.py
def test_end_to_end_query(agent):
    response = agent.process_query("Show total sales by region")
    
    assert response.success == True
    assert response.sql_query is not None
    assert "SELECT" in response.sql_query
    assert response.insights != ""
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific test
pytest test_core.py::test_sql_generation -v

# Run integration tests only
pytest tests/ -k integration -v
```

---

## Deployment

### Local Development

```bash
# Streamlit UI
streamlit run app.py

# CLI
python main.py

# Run examples
python example_usage.py
```

### Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]
```

```bash
# Build and run
docker build -t databricks-agent .
docker run -p 8501:8501 --env-file .env databricks-agent
```

### Databricks Deployment

```python
# Upload to DBFS
dbutils.fs.cp("file:/path/to/app", "dbfs:/apps/insight-agent/", recurse=True)

# Create job
{
  "name": "Insight Agent",
  "tasks": [{
    "task_key": "run_agent",
    "python_file": "dbfs:/apps/insight-agent/main.py",
    "cluster_spec": {...}
  }]
}
```

---

## Best Practices

### Code Quality

1. **Use Type Hints**
```python
def generate_sql(
    self,
    table_name: str,
    columns: Optional[List[str]] = None
) -> Optional[str]:
    ...
```

2. **Document Functions**
```python
def process_query(self, user_query: str) -> AgentResponse:
    """
    Process a user query end-to-end.
    
    Args:
        user_query: Natural language query from user
        
    Returns:
        AgentResponse with results and insights
        
    Raises:
        ValueError: If query is empty
        SecurityError: If query violates security rules
    """
```

3. **Error Handling**
```python
try:
    results = self.databricks_client.execute_query(sql)
except ConnectionError as e:
    logger.error(f"Database connection failed: {e}")
    return error_response("Unable to connect to database")
except Exception as e:
    logger.exception("Unexpected error")
    return error_response("An unexpected error occurred")
```

### Performance Optimization

1. **Cache Embeddings**
```python
@lru_cache(maxsize=1000)
def get_embedding(text: str) -> np.ndarray:
    return model.encode(text)
```

2. **Batch Processing**
```python
# Bad: Process one by one
for doc in documents:
    embedding = model.encode(doc)
    
# Good: Batch process
embeddings = model.encode(documents, batch_size=32)
```

3. **Optimize Delta Tables**
```sql
-- Regular maintenance
OPTIMIZE sales_table ZORDER BY (date, region);
VACUUM sales_table RETAIN 168 HOURS;
```

### Security

1. **Never Log Sensitive Data**
```python
# Bad
logger.info(f"User query: {query} with token: {token}")

# Good
logger.info(f"Processing query for user: {user_id}")
```

2. **Validate All Inputs**
```python
if not query or len(query) > MAX_LENGTH:
    raise ValueError("Invalid query length")

if not re.match(r'^[a-zA-Z0-9\s\-_.,?]+$', query):
    raise ValueError("Query contains invalid characters")
```

3. **Use Environment Variables**
```python
# Bad
token = "dapi1234567890"

# Good
token = os.getenv('DATABRICKS_ACCESS_TOKEN')
if not token:
    raise ConfigurationError("Missing access token")
```

---

## Troubleshooting

### Common Issues

**Issue: "FAISS index not loading"**
```bash
# Solution: Rebuild index
rm data/faiss_index.faiss
python -c "from context_retriever import *; rebuild_index()"
```

**Issue: "SQL generation returns None"**
```python
# Check schema is loaded
print(schema_manager.get_all_tables())

# Verify table structure
print(schema_manager.get_table('sales').columns)
```

**Issue: "Rate limit exceeded"**
```python
# Adjust rate limits in .env
RATE_LIMIT_PER_MINUTE=120

# Or clear rate limiter state
rate_limiter.reset()
```

---

**Version:** 2.0.0  
**Last Updated:** January 11, 2026  
**Maintainers:** @HasnainShafiq98
